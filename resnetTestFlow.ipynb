{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'resnet50'\n",
    "url, imgFile = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "url, classFile = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "try: urllib.URLopener().retrieve(url, imgFile)\n",
    "except: urllib.request.urlretrieve(url, imgFile)\n",
    "\n",
    "# Download ImageNet labels\n",
    "import urllib\n",
    "try: urllib.URLopener().retrieve(url, classFile)\n",
    "except: urllib.request.urlretrieve(url, classFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', model_file, pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "model.eval()\n",
    "torch.save(model, model_file + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = torch.load(model_file + '.pth')\n",
    "model = model.eval().to(device)\n",
    "\n",
    "input = torch.randn((1, 3, 224, 224), dtype=torch.float32).to(device)\n",
    "input = input.to(device)\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"input\": {0: \"batch_size\"},\n",
    "    \"output\": {0: \"batch_size\"}\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input,\n",
    "    model_file + \".onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=11,\n",
    ")\n",
    "\n",
    "print(\"onnx model export success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.tools.make_dynamic_shape_fixed --dim_param batch_size --dim_value 1 \"resnet50.onnx\" \"resnet50_shape1.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "options = onnxruntime.SessionOptions()  \n",
    "model_path = Path(model_file + '_shape1.onnx')\n",
    "onnx_ctx_path = model_path.with_suffix('.onnx_ctx.onnx')  \n",
    "print(onnx_ctx_path)\n",
    "if os.path.exists(onnx_ctx_path):      \n",
    "    model_path = onnx_ctx_path    \n",
    "else:      \n",
    "    options.add_session_config_entry('ep.context_enable', '1')      \n",
    "    options.add_session_config_entry('ep.context_embed_mode', '0')\n",
    "# (Optional) Enable configuration that raises an exception if the model can't be\n",
    "# run entirely on the QNN HTP backend.\n",
    "options.add_session_config_entry(\"session.disable_cpu_ep_fallback\", \"1\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(model_path, \n",
    "                                       sess_options=options,\n",
    "                                       providers=[\"QNNExecutionProvider\"],\n",
    "                                       provider_options=[{\"backend_path\": \"QnnHtp.dll\"}])\n",
    "# ort_session = onnxruntime.InferenceSession(model_file + '.onnx', providers=['CPUExecutionProvider'])\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = ort_session.get_outputs()[0].name\n",
    "print(input_name, output_name)\n",
    "print(\"Available providers:\", ort_session.get_providers())\n",
    "print(\"Current provider:\", ort_session.get_provider_options())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_image = Image.open(imgFile)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0).numpy() # create a mini-batch as expected by the model\n",
    "\n",
    "\n",
    "print(input_tensor.shape)\n",
    "print(input_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for i in range(1000): #Inferencing 1000 times makes the NPU run for a few seconds.\n",
    "    pred_logits = ort_session.run([output_name], {input_name: input_batch})[0]\n",
    "    pred_logits = torch.tensor(pred_logits)\n",
    "    pred_softmax = F.softmax(pred_logits, dim=1)\n",
    "    top5_prob, top5_catid = torch.topk(pred_softmax, 5)\n",
    "    top5_prob = top5_prob[0]\n",
    "    top5_catid = top5_catid[0]\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
